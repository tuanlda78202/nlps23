# GPT2 base: n_layers=12, n_heads=12, n_embeds=768 ~ 124M parameters
batch_size: 8 

eval_iters: 500 
eval_only: True

wandb_log: False
init_from: "gpt2"