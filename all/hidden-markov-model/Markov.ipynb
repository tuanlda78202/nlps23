{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "gz2Y6EhGCts2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import string"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK0B-TPHCts3"
      },
      "source": [
        "## Section-1: Dataset Manipulation\n",
        "---\n",
        "The following main goals are achieved in this section:\n",
        "  - Dataset is loaded\n",
        "  - Uninteresting lines are deleted from the dataset\n",
        "  - Lines are converted into a list of string for further processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KwkmeHMPCts3",
        "outputId": "1239a75e-0ff7-4097-fa50-2f3aa6ae0035"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset csv (/home/pham-son/.cache/huggingface/datasets/phamson02___csv/phamson02--vietnamese-poetry-corpus-8d629a9d62b1c63b/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
            "100%|██████████| 1/1 [00:00<00:00, 295.02it/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"phamson02/vietnamese-poetry-corpus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>genre</th>\n",
              "      <th>period</th>\n",
              "      <th>specific_genre</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>đẩy hoa dun lá khỏi tay trời , &lt;\\n&gt; nghĩ lại t...</td>\n",
              "      <td>Gửi cô Trương Quỳnh Như bài 2</td>\n",
              "      <td>https://www.thivien.net/</td>\n",
              "      <td>bảy chữ</td>\n",
              "      <td>Tây Sơn</td>\n",
              "      <td>thất ngôn bát cú</td>\n",
              "      <td>Phạm Thái</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>từ chốn thềm cung trộm dấu hương , &lt;\\n&gt; dễ xui...</td>\n",
              "      <td>Gửi cô Trương Quỳnh Như bài 1</td>\n",
              "      <td>https://www.thivien.net/</td>\n",
              "      <td>bảy chữ</td>\n",
              "      <td>Tây Sơn</td>\n",
              "      <td>thất ngôn bát cú</td>\n",
              "      <td>Phạm Thái</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hợi sang xanh biếc ngọn đèn tàn , &lt;\\n&gt; gượng đ...</td>\n",
              "      <td>Giờ hợi</td>\n",
              "      <td>https://www.thivien.net/</td>\n",
              "      <td>bảy chữ</td>\n",
              "      <td>Tây Sơn</td>\n",
              "      <td>thất ngôn bát cú</td>\n",
              "      <td>Phạm Thái</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>giờ tuất canh sang được mấy phần , &lt;\\n&gt; càng t...</td>\n",
              "      <td>Giờ tuất</td>\n",
              "      <td>https://www.thivien.net/</td>\n",
              "      <td>bảy chữ</td>\n",
              "      <td>Tây Sơn</td>\n",
              "      <td>thất ngôn bát cú</td>\n",
              "      <td>Phạm Thái</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>trời xanh cao thẳm mấy tầng khơi , &lt;\\n&gt; nỡ để ...</td>\n",
              "      <td>Khóc cô Trương Quỳnh Như bài 1</td>\n",
              "      <td>https://www.thivien.net/</td>\n",
              "      <td>bảy chữ</td>\n",
              "      <td>Tây Sơn</td>\n",
              "      <td>thất ngôn bát cú</td>\n",
              "      <td>Phạm Thái</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  \\\n",
              "0  đẩy hoa dun lá khỏi tay trời , <\\n> nghĩ lại t...   \n",
              "1  từ chốn thềm cung trộm dấu hương , <\\n> dễ xui...   \n",
              "2  hợi sang xanh biếc ngọn đèn tàn , <\\n> gượng đ...   \n",
              "3  giờ tuất canh sang được mấy phần , <\\n> càng t...   \n",
              "4  trời xanh cao thẳm mấy tầng khơi , <\\n> nỡ để ...   \n",
              "\n",
              "                            title                       url    genre   period  \\\n",
              "0   Gửi cô Trương Quỳnh Như bài 2  https://www.thivien.net/  bảy chữ  Tây Sơn   \n",
              "1   Gửi cô Trương Quỳnh Như bài 1  https://www.thivien.net/  bảy chữ  Tây Sơn   \n",
              "2                         Giờ hợi  https://www.thivien.net/  bảy chữ  Tây Sơn   \n",
              "3                        Giờ tuất  https://www.thivien.net/  bảy chữ  Tây Sơn   \n",
              "4  Khóc cô Trương Quỳnh Như bài 1  https://www.thivien.net/  bảy chữ  Tây Sơn   \n",
              "\n",
              "     specific_genre     author  \n",
              "0  thất ngôn bát cú  Phạm Thái  \n",
              "1  thất ngôn bát cú  Phạm Thái  \n",
              "2  thất ngôn bát cú  Phạm Thái  \n",
              "3  thất ngôn bát cú  Phạm Thái  \n",
              "4  thất ngôn bát cú  Phạm Thái  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(dataset[\"train\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH-tjDDuMqT1",
        "outputId": "5640529f-b01d-48c2-bfc4-6f5f071c6704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bảy chữ', 'thơ tự do', 'lục bát', 'năm chữ', 'bốn chữ', 'tám chữ', 'sáu chữ']\n"
          ]
        }
      ],
      "source": [
        "genre_list = df['genre'].tolist()\n",
        "genre_list = list( dict.fromkeys(genre_list) )\n",
        "\n",
        "print(genre_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1i2VF--Cts4",
        "outputId": "a0aff9d1-cbfe-496a-cc2a-88c943f8f56a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "89943\n"
          ]
        }
      ],
      "source": [
        "df = df.loc[df['genre'] == 'lục bát']\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4_JOV7MReT9",
        "outputId": "8d63a4b4-f907-471d-8b8a-5312b6d497ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10793\n"
          ]
        }
      ],
      "source": [
        "df = df.sample(frac=0.12)\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49hebuAwCts4",
        "outputId": "bc777aa6-9c13-4e5e-d21e-7d5e69a0ebd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tiếng ve rền dĩ hè về <\\n> mùa hoa phượng đỏ gọi hè hai mươi <\\n> mênh mang thân thiện nụ cười <\\n> về vui nghỉ mát tình người quê em <\\n> <\\n> biết bao nét đẹp mà xem <\\n> tấm hình kỷ niệm ai đem hữu tình <\\n> nhớ về quê mẹ thân thương <\\n> gợi thương gợi nhớ quê hương bến làng', 'em là con gái nhà quê <\\n> quanh năm cày cấy sớm khuya ruộng vườn <\\n> tay bùn trân nấm sớm hôm <\\n> chẳng biết thành thị phồn vinh thế nào', 'ta thương phận đá mồ côi <\\n> cô đơn xanh giữa cõi đời mênh mông <\\n> <\\n> đời lăn lóc đá buồn không <\\n> hoang vu gió quất cho lòng đá đau <\\n> <\\n> thề nguyền tạc đến ngàn sau <\\n> trăm năm lỗi hẹn lạc nhau có về <\\n> <\\n> đá mòn mỏi đá tái tê <\\n> ta ru đá đá ru về thiên thu', 'trâu vàng gõ cửa từng nhà <\\n> mùa màng tươi tốt thế là ấm no <\\n> của cải làm được đầy kho <\\n> tiền vào đầy két trời cho kiếm tiền <\\n> <\\n> đầu năm chồng vợ cưới liền <\\n> bảo nhau tích cưc kiếm tiền thật chăm <\\n> hêt dịch xây lầu cuối năm <\\n> nội thất xắm đủ rồi chăm thằng dần <\\n> <\\n> xe cộ nếu xét thấy cần <\\n> vài ba năm nữa tinh thân sẽ mua <\\n> không cần phải vội ganh đua <\\n> sinh con nuôi dạy nắng mưa trong nhà <\\n> <\\n> của cải rộng rãi có đà <\\n> con cái ăn học đồng ra đồng vào <\\n> vợ chồng đây đó cao trào <\\n> mua xe quan hệ ngọt ngào tình thân <\\n> <\\n> trâu vàng cày kéo chuyên cần <\\n> chủ giàu trâu khỏe xoay vần sẽ xong <\\n> năm trâu trâu chỉ cầu mong <\\n> chủ đươc hạnh phúc nhà đông của nhiều', 'người ơi nơi đó lạnh không <\\n> nhớ em cô bé má hồng tươi xinh <\\n> với anh em đẹp vô hình <\\n> nụ cười duyên dáng lung linh tuyệt vời <\\n> ngỡ đâu duyên phận do trời <\\n> đang chung mộng ước em đành rời xa <\\n> nhớ em đau lắm em à <\\n> tình yêu ngày ấy như là hư không <\\n> vắng em anh mãi đứng trông <\\n> biết bao ngày tháng mặn nồng thiết tha <\\n> em yêu anh lắm cơ mà <\\n> trời xanh sao để đôi ta chia lìa <\\n> mất em lệ chảy đầm đìa <\\n> dệt thêu tình nghĩa giờ này đơn phương <\\n> cách xa đâu phải chặng đường <\\n> mà hai thế giới khói hương mịt mờ <\\n> nỗi buồn anh viết thành thơ <\\n> em đi anh sống bơ vơ một mình <\\n> căn phòng lạnh ngắt lặng thinh <\\n> thiếu người chung sống thiếu tình đơn côi <\\n> mùa noel đó qua rồi <\\n> nhìn em tôi khóc bóng hình khuất xa <\\n> chỉ ta còn lại mình ta <\\n> trời đông buốt giá trong lòng lạnh căm <\\n> giữa đêm trăng sáng anh nằm <\\n> sao yên giấc ngủ năm rồi em đi <\\n> mất em anh biết làm gì <\\n> khi duyên đã hết đường tình chia đôi <\\n> trách sao phận bạc như vôi <\\n> cướp người con gái tuyệt vời tôi thương <\\n> giáng sinh năm đó chung đường <\\n> năm nay em đã về phương cõi trời <\\n> hai hàng lệ ứa châu rơi <\\n> yêu em yêu mãi trọn đời không quên <\\n> trong mơ anh vẫn nhắc tên <\\n> nhiều đêm cứ ngỡ em bên cạnh mình <\\n> dù không phải mối tình đầu <\\n> nhưng yêu em cũng đã lâu lắm rồi <\\n> hôm nay anh nhớ anh ngồi <\\n> cầu mong ta sẽ luân hồi kiếp sau']\n"
          ]
        }
      ],
      "source": [
        "lines = df['content'].tolist ()\n",
        "print(lines [:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "itFjTFrOCts5"
      },
      "outputs": [],
      "source": [
        "# The length of lines generated based on HMM is restricted to this value.\n",
        "# It is a safeguard to protect the code from a potential infinite loop\n",
        "MAX_LINE_LENGTH = 8\n",
        "\n",
        "# This is used to check the equality of two floating point values. If\n",
        "# their difference is less than this value, they are considered equal\n",
        "FLOAT_DELTA = 0.0001\n",
        "\n",
        "# This special string is used to mark the end of a sentence\n",
        "END_TOKEN = 'endl'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "18KNnIUlCts5"
      },
      "source": [
        "### Section-2.2: Helper Functions for Text-Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "NQEI3q8sCts5"
      },
      "outputs": [],
      "source": [
        "# Remove white-spaces and punctuations from a line and convert it\n",
        "# into a list of tokens\n",
        "def tokenize (line):\n",
        "    baseline = line.strip ().lower ()\n",
        "    tokens = ''.join ([x for x in baseline if x not in string.punctuation]).split ()\n",
        "    return tokens\n",
        "\n",
        "# Add a pairing to dictionary\n",
        "def insert_link (dictionary, key, value, debug = False):\n",
        "    if key not in dictionary:\n",
        "        dictionary [key] = []\n",
        "    if debug: print(key, dictionary [key])\n",
        "    dictionary [key].append (value)\n",
        "\n",
        "# Convert list to probability values\n",
        "def to_probability (chain):\n",
        "    frequencies = {}\n",
        "    probabilities = {}\n",
        "    num_of_words = len (chain)\n",
        "\n",
        "    for word in chain:\n",
        "        frequencies [word] = frequencies.get (word, 0) + 1\n",
        "\n",
        "    for word, frequency in frequencies.items ():\n",
        "        probabilities [word] = round (float (frequency) / num_of_words, 3)\n",
        "\n",
        "    return probabilities"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IM4FiGOrCts5"
      },
      "source": [
        "### Section-2.3: Main Function for Building the Markov Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "bnKvyf5CCts6"
      },
      "outputs": [],
      "source": [
        "def build_markov_model (corpus, first_order_markov_chain, second_order_markov_chain):\n",
        "    # This is a dictionary of words which have been used to\n",
        "    # start a line in Shakespeare's plays\n",
        "    words = []\n",
        "\n",
        "    for line in corpus:\n",
        "        tokens = tokenize (line)\n",
        "        num_of_tokens = len (tokens)\n",
        "\n",
        "        for idx in range (num_of_tokens):\n",
        "            token = tokens [idx]\n",
        "\n",
        "            if idx == 0:\n",
        "                words.append (token)\n",
        "\n",
        "                # We are not interested in the first word of a\n",
        "                # line since nothing precedes it\n",
        "                continue\n",
        "\n",
        "            # Populate first-order markov chain\n",
        "            last_token = tokens [idx - 1]\n",
        "            insert_link (first_order_markov_chain, last_token, token)\n",
        "\n",
        "            # The second word in a line can only have a first-level\n",
        "            # markov chain since there is only a single word before it\n",
        "            if idx == 1:\n",
        "                continue\n",
        "\n",
        "            # The last pair of word of a line is special. We want\n",
        "            # to chain it with 'END'; to help in finishing a line\n",
        "            # during predicitions\n",
        "            if idx == num_of_tokens - 1:\n",
        "                insert_link (second_order_markov_chain, (last_token, token), END_TOKEN)\n",
        "\n",
        "            # Populate second-order markov chain\n",
        "            second_last_token = tokens [idx - 2]\n",
        "            insert_link (second_order_markov_chain, (second_last_token, last_token), token)\n",
        "\n",
        "    # Convert first-order markov chain to probability values\n",
        "    for word, chain in first_order_markov_chain.items ():\n",
        "        first_order_markov_chain [word] = to_probability (chain)\n",
        "\n",
        "    # Convert first-order markov chain to probability values\n",
        "    for pair, chain in second_order_markov_chain.items ():\n",
        "        second_order_markov_chain [pair] = to_probability (chain)\n",
        "\n",
        "    print ('[STATUS] Successfully built Markov Model on Corpus!\\n')\n",
        "    return list (set (words))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aFZ-xxmrCts6"
      },
      "source": [
        "### Section-2.4: Helpers Functions for using the Markov Model for Text-Generation from the Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_Gx3hEwICts6"
      },
      "outputs": [],
      "source": [
        "# Pick next word from the second-order markov chain. It should be the\n",
        "# highest probability one. If multiple such words exist, randomly pick one\n",
        "def predict_next_word (key, dictionary, debug = False):\n",
        "    max_probability = 0.0\n",
        "    most_probable_words = []\n",
        "\n",
        "    for next_word, probability in dictionary.items ():\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            most_probable_words = [next_word]\n",
        "        elif max_probability - probability < FLOAT_DELTA:\n",
        "            most_probable_words.append (next_word)\n",
        "\n",
        "    if debug: print (key, most_probable_words)\n",
        "    return random.choice (most_probable_words)\n",
        "\n",
        "# Randomly pick a word that can follow; from the first-order markov chain\n",
        "def pick_next_word (key, dictionary, debug = False):\n",
        "    if debug: print(dictionary)\n",
        "    return random.choice (list(dictionary.keys()))\n",
        "\n",
        "# Generate text based on corpus\n",
        "def write_line (start_word, markov_chain_one, markov_chain_two):\n",
        "    line = []\n",
        "    word = start_word.lower ()\n",
        "\n",
        "    if word not in markov_chain_one.keys ():\n",
        "        return 0\n",
        "\n",
        "    line.append (word)\n",
        "    next_word = pick_next_word (start_word, markov_chain_one [start_word])\n",
        "    line.append (next_word)\n",
        "\n",
        "    n = 0\n",
        "    while n < MAX_LINE_LENGTH:\n",
        "        next_next_word = predict_next_word ((word, next_word), markov_chain_two [(word, next_word)])\n",
        "\n",
        "        if next_next_word == END_TOKEN:\n",
        "            return ' '.join (line)\n",
        "\n",
        "        word = next_word\n",
        "        next_word = next_next_word\n",
        "        line.append (next_next_word)\n",
        "        n += 1\n",
        "\n",
        "# Write a Shakespeare play of given length\n",
        "def write_play (hints, mc1, mc2):\n",
        "    string_generated = \"\"\n",
        "    for word in hints:\n",
        "        line = write_line (word, mc1, mc2)\n",
        "        if (line):\n",
        "          line += \"\\n\"\n",
        "          string_generated += line\n",
        "    return string_generated"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dEXeFcxCCts7"
      },
      "source": [
        "### Section-2.5: User API for Text Prediction Given a Sequence of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "id": "OLjpnCfiCts7"
      },
      "outputs": [],
      "source": [
        "def predict_next (sequence, mc1, mc2):\n",
        "    # Sanity checks\n",
        "    sequence = sequence.strip ()\n",
        "    if (sequence == \"\"):\n",
        "        raise ValueError('Sequence must not be an empty string. Please retry!')\n",
        "\n",
        "    tokens = tokenize (sequence)\n",
        "    line = ''\n",
        "    for token in reversed (tokens):\n",
        "        line = write_line (token, mc1, mc2)\n",
        "\n",
        "        if line:\n",
        "            break\n",
        "\n",
        "    return line + \"\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rm_YyajCts7",
        "outputId": "760311e2-c316-45b3-ead3-44e58cc36eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[STATUS] Successfully built Markov Model on Corpus!\n",
            "\n",
            "CPU times: user 2.84 s, sys: 25.1 ms, total: 2.87 s\n",
            "Wall time: 2.86 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# This is the first order markov chain. It chains a word\n",
        "# with the word(s) that can come after it\n",
        "mc_odr1 = {}\n",
        "\n",
        "# This is the second order markov chain. It chains a pair\n",
        "# of words with word(s) that can follow it\n",
        "mc_odr2 = {}\n",
        "\n",
        "words = build_markov_model(lines, mc_odr1, mc_odr2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1oOmT-6Cts7",
        "outputId": "9778be65-b0cb-400d-e56a-8eebe1163bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chưa ánh mắt nụ cười của em\n",
            "vườn nghĩ ra phận mình\n",
            "vụng nhớ thầm nhớ trộm ai hay\n",
            "nún môi gọt bút phiêu rằm thơ tình\n",
            "quà ngày này năm xưa\n",
            "buông mát mẻ đêm trường nhớ ai\n",
            "ngõ cây đào trước gió đung đưa\n",
            "chúc huynh tỷ muội tặng nàng\n",
            "nẻo xưa nhà bên nhau\n",
            "vọng đẩy lui nỗi buồn\n",
            "mỉm cười em đến bên tôi\n",
            "tre xạc xào lá rụng rơi giọt buồn\n",
            "cây bút viết nhiều lời thơ yêu thương\n",
            "biên giữ cho em\n",
            "cưới tươi hồng ban mai\n",
            "buồn ước mơ\n",
            "ngôi vào giữa lòng đất mẹ quê hương\n",
            "trên áo cồn cào nhớ nhung\n",
            "thảng vừa nhớ nhung\n",
            "xõa lòng anh vẫn nhớ vẫn chờ đợi ai\n",
            "bát rộn ràng tiếng trống xa cắc tòm\n",
            "nông cụ lao động sớm hôm chăm làm vui\n",
            "nợ trăng gió ngàn đêm cho vơi nỗi niềm\n",
            "đoàn cháu con\n",
            "nông hợp lực ta cùng chung một nhà\n",
            "chè cũng đàn bà ” cho đời thêm vui\n",
            "ngẩn cam sành lột luôn chiều lên cao\n",
            "thoắt đường dài em đi để lại cho người\n",
            "suối rong chơi bên mẹ hiền\n",
            "bằng trợ giáo rạng danh nước nhà\n",
            "ngân vẳng câu hát ngọt ngào\n",
            "quay về\n",
            "chẳng thành duyên tơ hồng se duyên tình\n",
            "búp bê thương đau yêu nhau\n",
            "vao trong mơ em đi để lại cho người\n",
            "uốn tháng ngày\n",
            "phá hỏng cơ đồ\n",
            "quay trở lại quê hương\n",
            "đượm giờ đây em có biết không\n",
            "niềm tin\n",
            "muộn của em\n",
            "cỏ thời gian\n",
            "đã nhúng chàm dám yêu thương\n",
            "thở tiếng sầu nhớ thương\n",
            "mắc em nổi sóng mù khơi lạnh lùng\n",
            "ê vàng thu\n",
            "men nắng ngọt ngào\n",
            "xuất giá câu thề ngày xưa\n",
            "miệt thứ nên thơ\n",
            "gặp bác hồ\n",
            "kể kiếp xa xưa\n",
            "má phía sau nỗi nhớ mong\n",
            "nơi ruột rà cùng chung một nhà\n",
            "phía thật là vui ghê xôn xao\n",
            "lượt người thân yêu thương\n",
            "vầng xẻ nửa vầng trăng khuyết trăng\n",
            "níu gót thi nhân\n",
            "cung hờn khúc duyên tình\n",
            "sắp bị thất sũng lại liền lên cao\n",
            "\n"
          ]
        }
      ],
      "source": [
        "play_length = 150\n",
        "hints = [random.choice (words) for x in range (play_length)]\n",
        "poem = write_play (hints, mc_odr1, mc_odr2)\n",
        "print(poem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peWtcuWyTH6p",
        "outputId": "46f42573-4b3b-42d1-ecda-52d7ce0517e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "348\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "print(len(re.findall(r'\\w+', poem)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "4c92415245794a369ae0485edb4430c1",
            "7fb9a704d0c0457cac9dc393afaa80d8",
            "8f2b785d4b0b4f9eb213edaa8d3d5e5d",
            "1260984fa71748e5944be41749c787c2",
            "f177ea644b3f4234bde22c805e72eb77",
            "390173fc214342679f2343c61621bba1",
            "d1a85482af0d4d8e860f10d433cd2321",
            "3f5b3b9599e54a6e8850421f507d2857",
            "ebf92cedb59c400994a3f0c89540cb76",
            "f94523eb78b84800ab7040690d30f70b",
            "b0a626e4810b4e5980a6daf36b252a2a"
          ]
        },
        "id": "jLqBPp2DCv2j",
        "outputId": "4f0f5ff6-b16f-419c-89e8-25337f8aa5f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "đống tim hồng của em.cấm mất rồi.thức chưa quên một người.đồ môn nghệ thuật chuyên chuân vẹn tròn.lãng chứa chan tình người.với ngổn ngang trăm mối tơ vò với ai.cửu trên đời chẳng quên.hè bốn mươi năm lẻ bóng cô đơn.cắm xuống lại ngày xưa.quanh rối rắm tơ vò với ai.phan thi nữ ngọt ngào.sầu hay là em đã có chồng hay chưa.sắp biệt nhớ nhung.nào sen quê hương.làng ngày xưa.đỏ năm xưa.rau giỗ người.rã nhau.buổi đoạn trường ai hay.nửa khuyết tái tê cõi lòng.tha kiếp này.thím đương thì.lối đê trót vì thơ.hết em về với mẹ cha.tới mình tuổi thơ.níu bồi hồi nhớ thương.đầm để dành cho nhau.chèo xưa hát xứ đông cho ta nhớ người.san tả tơi thân xác héo hon.ngót nghét tám mươi năm lẻ bóng cô liêu.vu khi thân nát hồn đau con tim.tủi chẳng ai có nhớ không tên thuở nào.kiếp loài ta nghĩa tình.phím chừ nằm nỗi đau.củ mì từ đây.dáng những hình hài của con.cành nguyên trinh cho đời thêm vui.nghe lấy những lời yêu thương.rung tâm hồn.nhánh thương đau yêu nhau.cuộn tràn tình thương dạt dào tình thơ.hồng chéo ngoe.nhấm chút tình cho em.xé tà trăng xưa đã có chồng hay chưa.chăn cầu mong mẹ về cõi tiên.khát phù sa bồi đắp cho em.phận nẻo đời ước mơ.nỡ so bì với ai.cởi mở nối liền hai ta.cải năm nào.ngả nong lời quê.chắt thời gian.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading builder script: 100%|██████████| 8.46k/8.46k [00:00<00:00, 7.95MB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 1.68G/1.68G [12:38<00:00, 2.22MB/s]\n",
            "Some weights of the model checkpoint at vinai/bartpho-word were not used when initializing MBartForCausalLM: ['encoder.layers.9.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.6.fc1.weight', 'encoder.embed_positions.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.7.self_attn_layer_norm.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.11.fc1.weight', 'encoder.layers.6.self_attn_layer_norm.bias', 'encoder.layers.10.self_attn_layer_norm.weight', 'encoder.layers.6.self_attn_layer_norm.weight', 'encoder.layers.2.fc2.weight', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.6.fc1.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.11.self_attn_layer_norm.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.1.fc2.bias', 'encoder.layers.7.self_attn_layer_norm.weight', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.6.self_attn.k_proj.bias', 'encoder.layers.10.fc1.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.7.fc1.bias', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.3.fc1.weight', 'encoder.layers.9.fc2.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.8.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.3.fc2.bias', 'encoder.layers.9.self_attn_layer_norm.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.9.self_attn_layer_norm.bias', 'encoder.layers.11.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.10.fc2.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.6.fc2.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.8.fc1.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.9.fc2.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.6.fc2.bias', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.9.fc1.bias', 'encoder.layers.10.self_attn.k_proj.bias', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.embed_tokens.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.9.self_attn.k_proj.bias', 'encoder.layers.1.fc1.bias', 'encoder.layers.9.fc1.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.4.fc1.weight', 'encoder.layers.10.fc2.bias', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.5.fc1.weight', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.k_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.8.fc1.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.11.fc2.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.0.fc2.bias', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.11.fc1.bias', 'encoder.layers.2.fc2.bias', 'encoder.layernorm_embedding.weight', 'encoder.layers.8.fc2.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.0.final_layer_norm.bias', 'shared.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.10.self_attn_layer_norm.bias', 'encoder.layers.0.fc1.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.1.fc2.weight', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.8.self_attn_layer_norm.bias', 'encoder.layer_norm.bias', 'encoder.layers.7.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.8.fc2.bias', 'encoder.layers.5.fc2.bias', 'encoder.layers.7.fc2.weight', 'encoder.layers.4.fc2.weight', 'encoder.layers.2.fc1.weight', 'encoder.layers.7.fc2.bias', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.1.fc1.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.11.fc2.bias', 'encoder.layers.11.self_attn_layer_norm.weight', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.10.fc1.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layernorm_embedding.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.7.fc1.weight', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.v_proj.weight']\n",
            "- This IS expected if you are initializing MBartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MBartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mprint\u001b[39m(list_of_string[\u001b[39m0\u001b[39m])\n\u001b[1;32m     57\u001b[0m list_of_string \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m a: a \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, list_of_string))\n\u001b[0;32m---> 59\u001b[0m perplexity \u001b[39m=\u001b[39m compute_perplexity(list_of_string)\n\u001b[1;32m     60\u001b[0m wordcount \u001b[39m=\u001b[39m compute_wordcount(list_of_string)\n\u001b[1;32m     61\u001b[0m diversity \u001b[39m=\u001b[39m compute_diversity(list_of_string)\n",
            "Cell \u001b[0;32mIn[22], line 15\u001b[0m, in \u001b[0;36mcompute_perplexity\u001b[0;34m(all_texts_list)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# max sequence length and batch size are limited to 256 and 2, respectively, to avoid OOM bug\u001b[39;00m\n\u001b[1;32m     14\u001b[0m resized_all_texts_list \u001b[39m=\u001b[39m [text[:\u001b[39m256\u001b[39m] \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m all_texts_list]\n\u001b[0;32m---> 15\u001b[0m results \u001b[39m=\u001b[39m perplexity\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49mresized_all_texts_list, model_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvinai/bartpho-word\u001b[39;49m\u001b[39m\"\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m \u001b[39mreturn\u001b[39;00m results[\u001b[39m'\u001b[39m\u001b[39mmean_perplexity\u001b[39m\u001b[39m'\u001b[39m]\n",
            "File \u001b[0;32m~/Workspace/nlps23/venv/lib/python3.10/site-packages/evaluate/module.py:444\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m inputs \u001b[39m=\u001b[39m {input_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    443\u001b[0m \u001b[39mwith\u001b[39;00m temp_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed):\n\u001b[0;32m--> 444\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompute_kwargs)\n\u001b[1;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--perplexity/8ab643ad86f568b7d1d5f7822373fa7401ff5ff0297ccf114b0ca6a33be96bc0/perplexity.py:115\u001b[0m, in \u001b[0;36mPerplexity._compute\u001b[0;34m(self, predictions, model_id, batch_size, add_start_token, device, max_length)\u001b[0m\n\u001b[1;32m    112\u001b[0m     device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m--> 115\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    117\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m    119\u001b[0m \u001b[39m# if batch_size > 1 (which generally leads to padding being required), and\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39m# if there is not an already assigned pad_token, assign an existing\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m# special token to also be the padding token\u001b[39;00m\n",
            "File \u001b[0;32m~/Workspace/nlps23/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1902\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1898\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1899\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1900\u001b[0m     )\n\u001b[1;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Workspace/nlps23/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
            "File \u001b[0;32m~/Workspace/nlps23/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/Workspace/nlps23/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/Workspace/nlps23/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/Workspace/nlps23/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/Workspace/nlps23/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
            "File \u001b[0;32m~/Workspace/nlps23/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    248\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import spacy\n",
        "import numpy as np\n",
        "import torch\n",
        "from evaluate import load\n",
        "from collections import defaultdict\n",
        "from nltk.util import ngrams\n",
        "\n",
        "\n",
        "def compute_perplexity(all_texts_list):\n",
        "    torch.cuda.empty_cache()\n",
        "    perplexity = load(\"perplexity\", module_type=\"metric\")\n",
        "    # max sequence length and batch size are limited to 256 and 2, respectively, to avoid OOM bug\n",
        "    resized_all_texts_list = [text[:256] for text in all_texts_list]\n",
        "    results = perplexity.compute(predictions=resized_all_texts_list, model_id=\"vinai/bartpho-word\", device='cuda', batch_size=2)\n",
        "    return results['mean_perplexity']\n",
        "\n",
        "def compute_wordcount(all_texts_list):\n",
        "    wordcount = load(\"word_count\")\n",
        "    wordcount = wordcount.compute(data=all_texts_list)\n",
        "    return wordcount['unique_words']\n",
        "\n",
        "\n",
        "def compute_diversity(all_texts_list):\n",
        "    ngram_range = [2, 3, 4]\n",
        "\n",
        "    tokenizer = spacy.load(\"en_core_web_sm\").tokenizer\n",
        "    token_list = []\n",
        "    for sentence in all_texts_list:\n",
        "        token_list.append([str(token) for token in tokenizer(sentence)])\n",
        "    ngram_sets = {}\n",
        "    ngram_counts = defaultdict(int)\n",
        "\n",
        "    metrics = {}\n",
        "    for n in ngram_range:\n",
        "        ngram_sets[n] = set()\n",
        "        for tokens in token_list:\n",
        "            ngram_sets[n].update(ngrams(tokens, n))\n",
        "            ngram_counts[n] += len(list(ngrams(tokens, n)))\n",
        "        metrics[f'{n}gram_repitition'] = (1 - len(ngram_sets[n]) / ngram_counts[n])\n",
        "    diversity = 1\n",
        "    for val in metrics.values():\n",
        "        diversity *= (1 - val)\n",
        "    metrics['diversity'] = diversity\n",
        "    return metrics\n",
        "\n",
        "list_of_string = []\n",
        "# generate words\n",
        "n = 256\n",
        "for i in range(150):\n",
        "    play_length = 150\n",
        "    hints = [random.choice (words) for x in range (play_length)]\n",
        "    poem = write_play (hints, mc_odr1, mc_odr2)\n",
        "    list_of_string.append(poem)\n",
        "list_of_string = [i.replace(\"\\n\", \".\") for i in list_of_string]\n",
        "print(list_of_string[0])\n",
        "list_of_string = list(filter(lambda a: a != \" \", list_of_string))\n",
        "\n",
        "perplexity = compute_perplexity(list_of_string)\n",
        "wordcount = compute_wordcount(list_of_string)\n",
        "diversity = compute_diversity(list_of_string)\n",
        "print(\"Perplexity:\", perplexity)\n",
        "print(\"Wordcount:\", wordcount)\n",
        "print(\"Diversity:\", diversity)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SgpUO-fYCts8"
      },
      "source": [
        "Following code demonstrates **text prediction given a sequence**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6-ZIyGpGyTH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1260984fa71748e5944be41749c787c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f94523eb78b84800ab7040690d30f70b",
            "placeholder": "​",
            "style": "IPY_MODEL_b0a626e4810b4e5980a6daf36b252a2a",
            "value": " 75/75 [00:07&lt;00:00, 10.19it/s]"
          }
        },
        "390173fc214342679f2343c61621bba1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f5b3b9599e54a6e8850421f507d2857": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c92415245794a369ae0485edb4430c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7fb9a704d0c0457cac9dc393afaa80d8",
              "IPY_MODEL_8f2b785d4b0b4f9eb213edaa8d3d5e5d",
              "IPY_MODEL_1260984fa71748e5944be41749c787c2"
            ],
            "layout": "IPY_MODEL_f177ea644b3f4234bde22c805e72eb77"
          }
        },
        "7fb9a704d0c0457cac9dc393afaa80d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_390173fc214342679f2343c61621bba1",
            "placeholder": "​",
            "style": "IPY_MODEL_d1a85482af0d4d8e860f10d433cd2321",
            "value": "100%"
          }
        },
        "8f2b785d4b0b4f9eb213edaa8d3d5e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f5b3b9599e54a6e8850421f507d2857",
            "max": 75,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebf92cedb59c400994a3f0c89540cb76",
            "value": 75
          }
        },
        "b0a626e4810b4e5980a6daf36b252a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1a85482af0d4d8e860f10d433cd2321": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebf92cedb59c400994a3f0c89540cb76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f177ea644b3f4234bde22c805e72eb77": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f94523eb78b84800ab7040690d30f70b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
